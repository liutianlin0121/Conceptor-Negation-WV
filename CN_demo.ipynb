{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Unsupervised Post-processing of Word Vectors via Conceptor Negation\n",
    "\n",
    "In this notebook, we presents the experiment results reported in [1]/\n",
    "\n",
    "[1] Unsupervised Post-processing of Word Vectors via Conceptor Negation. Tianlin Liu, Lyle Ungar, and João Sedoc, Unsupervised Post-processing of Word Vectors via Conceptor Negation, AAAI 2019.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import scipy, requests, codecs, os, re, nltk, itertools, csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
    "import tensorflow as tf\n",
    "from scipy.stats import spearmanr\n",
    "import pandas as pd\n",
    "import functools as ft\n",
    "\n",
    "# resourceFile = '/Users/liutianlin/Desktop/Academics/NLP/data/' \n",
    "resourceFile = '/data/' # the address of the datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install -q gdown\n",
    "!gdown https://drive.google.com/uc?id=1U_UGB2vyTuTIcbV_oeDtJCtAtlFMvXOM # download a small subset of glove\n",
    "!gdown https://drive.google.com/uc?id=1j_b4TRpL3f0HQ8mV17_CtOXp862YjxxB   # download a small subset of word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Word2Vec and GloVe word embeddings\n",
    " \n",
    "We provide a small word2vec and small glove word embedding in this repository -- their words appear at least 200 times in wikipedia (see the list provided by Arora et al https://github.com/PrincetonML/SIF/blob/master/auxiliary_data/enwiki_vocab_min200.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded Word2vec!\n",
      "loaded GloVe Common Crawl!\n"
     ]
    }
   ],
   "source": [
    "def loadWordVecs(model_str):\n",
    "    word_dictionary = {}\n",
    "    \n",
    "    input_file_destination = 'small_' + model_str + '.txt'\n",
    "\n",
    "    f = codecs.open(input_file_destination, 'r', 'utf-8') \n",
    "\n",
    "    for line in f:\n",
    "\n",
    "        line = line.split(\" \", 1)   \n",
    "        transformed_key = line[0].lower()\n",
    "\n",
    "        try:\n",
    "            transformed_key = str(transformed_key)\n",
    "\n",
    "        except:\n",
    "            print(\"Can't convert the key to unicode:\", transformed_key)\n",
    "\n",
    "        word_dictionary[transformed_key] = np.fromstring(line[1], dtype=\"float32\", sep=\" \")\n",
    "\n",
    "        if word_dictionary[transformed_key].shape[0] != 300:\n",
    "            print(transformed_key, word_dictionary[transformed_key].shape)\n",
    "\n",
    "    return  word_dictionary     \n",
    "\n",
    "\n",
    "\n",
    "orig_word2vec = loadWordVecs('word2vec')\n",
    "print(\"loaded Word2vec!\")\n",
    "\n",
    "orig_glove = loadWordVecs('glove')\n",
    "print(\"loaded GloVe Common Crawl!\")\n",
    "\n",
    "\n",
    "orig_model = {}\n",
    "orig_model['word2vec'] = orig_word2vec\n",
    "orig_model['glove'] = orig_glove\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Post-process Word2Vec and GloVe with Conceptor Negation (CN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-processing Word2vec with CN\n",
      "Post-processing GloVe with CN\n"
     ]
    }
   ],
   "source": [
    "def ensemble_cn_dict(wordVecModel_str, alpha = 2, orig_model = orig_model):\n",
    "    \n",
    "    \n",
    "    # put the word vectors in columns\n",
    "    x_collector = np.array(list(orig_model[wordVecModel_str].values())).T       \n",
    "        \n",
    "    \n",
    "    nrWords = x_collector.shape[1] # number of total words\n",
    "    \n",
    "    \n",
    "    R = x_collector.dot(x_collector.T) / nrWords # calculate the un-centered correlation matrix\n",
    "    \n",
    "    C = R @ np.linalg.inv(R + alpha ** (-2) * np.eye(300))# calculate the conceptor matrix\n",
    "    \n",
    "    vecMatrix = ((np.eye(300) - C) @ x_collector).T \n",
    "\n",
    "    cn_dict = {}\n",
    "        \n",
    "    for word_index in np.arange(0, len(orig_model[wordVecModel_str].keys())):\n",
    "        \n",
    "        word = list(orig_model[wordVecModel_str].keys())[word_index]\n",
    "        cn_dict[word] = vecMatrix[word_index,:]\n",
    "    \n",
    "    return cn_dict\n",
    "\n",
    "print(\"Post-processing Word2vec with CN\")\n",
    "cn_word2vec = ensemble_cn_dict('word2vec', orig_model = orig_model)\n",
    "\n",
    "print(\"Post-processing GloVe with CN\")\n",
    "cn_glove = ensemble_cn_dict('glove', orig_model = orig_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 1: Word similarity evaluation\n",
    "We evaluate the CN post-processed word vectors with 7 standard word similarity datasets: the RG65 (Rubenstein and Goodenough, 1965), the WordSim-353 (WS) (Finkelstein et al., 2002), the rare- words (RW) (Luong, Socher, and Manning, 2013), the MEN dataset (Bruni, Tran, and Baroni, 2014), the MTurk (Radinsky et al., 2011), the SimLex-999 (SimLex) (Hill, Reichart, and Korhonen, 2015), and the SimVerb-3500 (Gerz et al., 2016). \n",
    "\n",
    "To evaluate the word similarity, we calculate the cosine distance between vectors of two words. We report the Spearman’s rank correlation coefficient (Myers and Well, 1995) of the estimated rankings against the rankings given by human annotators.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataSets = ['EN-RG-65.txt', 'EN-WS-353-ALL.txt', 'EN-RW-STANFORD.txt', 'EN-MEN-TR-3k.txt', 'EN-MTurk-287.txt', 'EN-SIMLEX-999.txt', 'EN-SimVerb-3500.txt']\n",
    "\n",
    "\n",
    "\n",
    "def similarity_eval(dataSetAddress, wordVecModel_str):\n",
    "    wordVecModel = eval(wordVecModel_str)\n",
    "    vocab = set(list(wordVecModel.keys()))\n",
    "    \n",
    "    fread_simlex = open(dataSetAddress, \"r\")\n",
    "    \n",
    "    pair_list = []\n",
    "\n",
    "    line_number = 0\n",
    "    for line in fread_simlex:\n",
    "#         if line_number > 0:\n",
    "        tokens = line.split()\n",
    "        word_i = tokens[0]\n",
    "        word_j = tokens[1]\n",
    "        score = float(tokens[2])\n",
    "        if word_i in vocab and word_j in vocab:\n",
    "            pair_list.append( ((word_i, word_j), score) )\n",
    "#         line_number += 1\n",
    "\n",
    "    pair_list.sort(key=lambda x: - x[1]) # order the pairs from highest score (most similar) to lowest score (least similar)\n",
    "\n",
    "\n",
    "    extracted_scores = {}\n",
    "\n",
    "    extracted_list = []\n",
    "    \n",
    "               \n",
    "    for (x,y) in pair_list:\n",
    "        (word_i, word_j) = x\n",
    "        \n",
    "        current_distance = 1- cosine_similarity( wordVecModel[word_i].reshape(1,-1)  , wordVecModel[word_j].reshape(1,-1) )        \n",
    "\n",
    "        extracted_scores[(word_i, word_j)] = current_distance\n",
    "        extracted_list.append(((word_i, word_j), current_distance))\n",
    "\n",
    "    extracted_list.sort(key=lambda x: x[1])\n",
    "\n",
    "    spearman_original_list = []\n",
    "    spearman_target_list = []\n",
    "\n",
    "    for position_1, (word_pair, score_1) in enumerate(pair_list):\n",
    "        score_2 = extracted_scores[word_pair]\n",
    "        position_2 = extracted_list.index((word_pair, score_2))\n",
    "        spearman_original_list.append(position_1)\n",
    "        spearman_target_list.append(position_2)\n",
    "\n",
    "    spearman_rho = spearmanr(spearman_original_list, spearman_target_list)\n",
    "    \n",
    "    return spearman_rho[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating the data set EN-RG-65.txt\n",
      "Word2Vec + CN : 0.7972\n",
      "Glove + CN : 0.7913\n",
      "\n",
      "\n",
      "evaluating the data set EN-WS-353-ALL.txt\n",
      "Word2Vec + CN : 0.6926\n",
      "Glove + CN : 0.7886\n",
      "\n",
      "\n",
      "evaluating the data set EN-RW-STANFORD.txt\n",
      "Word2Vec + CN : 0.5804\n",
      "Glove + CN : 0.5898\n",
      "\n",
      "\n",
      "evaluating the data set EN-MEN-TR-3k.txt\n",
      "Word2Vec + CN : 0.7869\n",
      "Glove + CN : 0.8339\n",
      "\n",
      "\n",
      "evaluating the data set EN-MTurk-287.txt\n",
      "Word2Vec + CN : 0.6662\n",
      "Glove + CN : 0.7116\n",
      "\n",
      "\n",
      "evaluating the data set EN-SIMLEX-999.txt\n",
      "Word2Vec + CN : 0.4684\n",
      "Glove + CN : 0.4858\n",
      "\n",
      "\n",
      "evaluating the data set EN-SimVerb-3500.txt\n",
      "Word2Vec + CN : 0.3830\n",
      "Glove + CN : 0.3632\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "wordSimResult = {}\n",
    "\n",
    "\n",
    "for dataset in dataSets:\n",
    "    dataSetAddress = resourceFile + 'wordSimData/' +  dataset\n",
    "    print('evaluating the data set', dataset)\n",
    "    \n",
    "    print('Word2Vec + CN : %.4f' %  similarity_eval(dataSetAddress, 'cn_word2vec'))\n",
    "    print('Glove + CN : %.4f' %  similarity_eval(dataSetAddress, 'cn_glove'))\n",
    "        \n",
    "    print('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Experiment 2:  Semantic Textual Similarity (STS) tasks\n",
    "\n",
    "We use standard semantic textual similarity (STS) benchmarks to evaluate the post-processed word vectors: we use 2012-2015 SemEval STS tasks (Agirre et al., 2012, 2013, 2014, 2015) and 2012 SemEval Semantic Related task (SICK) (Marelli et al., 2014). \n",
    "\n",
    "We reuse the codes provided in  https://github.com/nlptown/nlp-notebooks/blob/master/Simple%20Sentence%20Similarity.ipynb\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year-task</th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>A plane is taking off.</td>\n",
       "      <td>An air plane is taking off.</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>A man is playing a large flute.</td>\n",
       "      <td>A man is playing a flute.</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>A man is spreading shreded cheese on a pizza.</td>\n",
       "      <td>A man is spreading shredded cheese on an uncoo...</td>\n",
       "      <td>3.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>Three men are playing chess.</td>\n",
       "      <td>Two men are playing chess.</td>\n",
       "      <td>2.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2012-MSRvid</td>\n",
       "      <td>A man is playing the cello.</td>\n",
       "      <td>A man seated is playing the cello.</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     year-task                                         sent_1  \\\n",
       "0  2012-MSRvid                         A plane is taking off.   \n",
       "1  2012-MSRvid                A man is playing a large flute.   \n",
       "2  2012-MSRvid  A man is spreading shreded cheese on a pizza.   \n",
       "3  2012-MSRvid                   Three men are playing chess.   \n",
       "4  2012-MSRvid                    A man is playing the cello.   \n",
       "\n",
       "                                              sent_2   sim  \n",
       "0                        An air plane is taking off.  5.00  \n",
       "1                          A man is playing a flute.  3.80  \n",
       "2  A man is spreading shredded cheese on an uncoo...  3.80  \n",
       "3                         Two men are playing chess.  2.60  \n",
       "4                 A man seated is playing the cello.  4.25  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_sts_dataset(filename):\n",
    "    # For a STS dataset, loads the relevant information: the sentences and their human rated similarity score.\n",
    "    sent_pairs = []\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        for line in f:\n",
    "            ts = line.strip().split(\"\\t\")\n",
    "            if len(ts) == 7 or len(ts) == 9:\n",
    "                sent_pairs.append((re.sub(\"[^0-9]\", \"\", ts[2]) + '-' + ts[1] , ts[5], ts[6], float(ts[4])))\n",
    "            elif len(ts) == 6 or len(ts) == 8:\n",
    "                sent_pairs.append((re.sub(\"[^0-9]\", \"\", ts[1]) + '-' + ts[0] , ts[4], ts[5], float(ts[3])))\n",
    "            else:\n",
    "                print('data format is wrong!!!')\n",
    "    return pd.DataFrame(sent_pairs, columns=[\"year-task\", \"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "\n",
    "def load_all_sts_dataset():\n",
    "    # Loads all of the STS datasets \n",
    "    stsbenchmarkDir = resourceFile + 'stsbenchmark/'\n",
    "    stscompanionDir = resourceFile + 'stsbenchmark/'\n",
    "    sts_train = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-train.csv\"))    \n",
    "    sts_dev = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-dev.csv\"))\n",
    "    sts_test = load_sts_dataset(os.path.join(stsbenchmarkDir, \"sts-test.csv\"))\n",
    "    sts_other = load_sts_dataset(os.path.join(stscompanionDir, \"sts-other.csv\"))\n",
    "    sts_mt = load_sts_dataset(os.path.join(stscompanionDir, \"sts-mt.csv\"))\n",
    "    \n",
    "    sts_all = pd.concat([sts_train, sts_dev, sts_test, sts_other, sts_mt ])\n",
    "    \n",
    "    return sts_all\n",
    "\n",
    "sts_all = load_all_sts_dataset()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_sts_by_year_task():\n",
    "    # Divide STS datasets based on their year and tasks\n",
    "    sts_by_year_task = {}\n",
    "    \n",
    "    for year_task in sts_all['year-task'].unique():\n",
    "        indices = [i for i, x in enumerate(list(sts_all['year-task'])) if x == year_task]\n",
    "        \n",
    "        pairs = sts_all.iloc[indices]\n",
    "        \n",
    "        sts_by_year_task[year_task] = pairs\n",
    "        \n",
    "    return sts_by_year_task\n",
    "\n",
    "sts_by_year_task = load_sts_by_year_task()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_sts_by_year():\n",
    "    # Divide STS datasets ONLY based on their year (different tasks in that year are merged).\n",
    "\n",
    "    sts_by_year = {}\n",
    "    \n",
    "    for year in ['2012', '2013', '2014', '2015', '2016', '2017']:\n",
    "        indices = [i for i, x in enumerate(list(sts_all['year-task'])) if x.startswith(year)]\n",
    "        \n",
    "        pairs = sts_all.iloc[indices]\n",
    "        pairs = pairs.copy()\n",
    "        pairs['year-task'] = year\n",
    "        sts_by_year[year] = pairs\n",
    "        \n",
    "    return sts_by_year\n",
    "\n",
    "sts_by_year_task = load_sts_by_year_task()\n",
    "\n",
    "sts_by_year = load_sts_by_year()\n",
    "\n",
    "\n",
    "filename = resourceFile + '2015-answers-students.test.tsv'\n",
    "sent_pairs = []\n",
    "with tf.gfile.GFile(filename, \"r\") as f:\n",
    "    for line in f:\n",
    "        ts = line.strip().split(\"\\t\")\n",
    "        if len(ts) == 3:\n",
    "            sent_pairs.append((ts[1], ts[2], float(ts[0])))\n",
    "answers_students_2015 =  pd.DataFrame(sent_pairs, columns=[\"sent_1\", \"sent_2\", \"sim\"])\n",
    "\n",
    "\n",
    "# show some sample sts data    \n",
    "sts_all[:5] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>sent_1</th>\n",
       "      <th>sent_2</th>\n",
       "      <th>sim</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>There is no boy playing outdoors and there is ...</td>\n",
       "      <td>A group of kids is playing in a yard and an ol...</td>\n",
       "      <td>3.300</td>\n",
       "      <td>NEUTRAL\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>A group of boys in a yard is playing and a man...</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>3.700</td>\n",
       "      <td>NEUTRAL\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>A group of children is playing in the house an...</td>\n",
       "      <td>The young boys are playing outdoors and the ma...</td>\n",
       "      <td>3.000</td>\n",
       "      <td>NEUTRAL\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>4.900</td>\n",
       "      <td>ENTAILMENT\\r</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>A brown dog is attacking another animal in fro...</td>\n",
       "      <td>A brown dog is helping another animal in front...</td>\n",
       "      <td>3.665</td>\n",
       "      <td>NEUTRAL\\r</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  idx                                             sent_1  \\\n",
       "0   6  There is no boy playing outdoors and there is ...   \n",
       "1   7  A group of boys in a yard is playing and a man...   \n",
       "2   8  A group of children is playing in the house an...   \n",
       "3  10  A brown dog is attacking another animal in fro...   \n",
       "4  11  A brown dog is attacking another animal in fro...   \n",
       "\n",
       "                                              sent_2    sim         label  \n",
       "0  A group of kids is playing in a yard and an ol...  3.300     NEUTRAL\\r  \n",
       "1  The young boys are playing outdoors and the ma...  3.700     NEUTRAL\\r  \n",
       "2  The young boys are playing outdoors and the ma...  3.000     NEUTRAL\\r  \n",
       "3  A brown dog is attacking another animal in fro...  4.900  ENTAILMENT\\r  \n",
       "4  A brown dog is helping another animal in front...  3.665     NEUTRAL\\r  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "def download_sick(f): \n",
    "\n",
    "    response = requests.get(f).text\n",
    "\n",
    "    lines = response.split(\"\\n\")[1:]\n",
    "    lines = [l.split(\"\\t\") for l in lines if len(l) > 0]\n",
    "    lines = [l for l in lines if len(l) == 5]\n",
    "\n",
    "    df = pd.DataFrame(lines, columns=[\"idx\", \"sent_1\", \"sent_2\", \"sim\", \"label\"])\n",
    "    df['sim'] = pd.to_numeric(df['sim'])\n",
    "    return df\n",
    "    \n",
    "sick_all = download_sick(\"https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_test_annotated.txt\")\n",
    "\n",
    "sick_all[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some preparation for STS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class Sentence:\n",
    "    \n",
    "    def __init__(self, sentence):\n",
    "        self.raw = sentence\n",
    "        normalized_sentence = sentence.replace(\"‘\", \"'\").replace(\"’\", \"'\")\n",
    "        self.tokens = [t.lower() for t in nltk.word_tokenize(normalized_sentence)]\n",
    "        \n",
    "def run_conceptor_benchmark(sentences1, sentences2, model_str): \n",
    "    \n",
    "    model = eval(model_str)\n",
    "    embeddings = []\n",
    "\n",
    "\n",
    "    for (sent1, sent2) in zip(sentences1, sentences2): \n",
    "\n",
    "        tokens1 =  sent1.tokens\n",
    "        tokens2 =  sent2.tokens\n",
    "\n",
    "        tokens1 = [token for token in tokens1 if token in model and token.islower()]\n",
    "        tokens2 = [token for token in tokens2 if token in model and token.islower()]\n",
    "\n",
    "        embedding1 = np.average([model[token] for token in tokens1], axis=0)\n",
    "        embedding2 = np.average([model[token] for token in tokens2], axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        if isinstance(embedding1, float) or isinstance(embedding2, float):\n",
    "            embeddings.append(np.zeros(300))\n",
    "            embeddings.append(np.zeros(300))\n",
    "        else:\n",
    "            embeddings.append(embedding1)\n",
    "            embeddings.append(embedding2)\n",
    "\n",
    "\n",
    "\n",
    "    sims = [cosine_similarity(embeddings[idx*2].reshape(1, -1), embeddings[idx*2+1].reshape(1, -1))[0][0] for idx in range(int(len(embeddings)/2))]\n",
    "    return sims\n",
    "\n",
    "def run_experiment(df, benchmarks): \n",
    "    \n",
    "    sentences1 = [Sentence(s) for s in df['sent_1']]\n",
    "    sentences2 = [Sentence(s) for s in df['sent_2']]\n",
    "    \n",
    "    pearson_cors, spearman_cors = [], []\n",
    "    for label, method in benchmarks:\n",
    "        sims = method(sentences1, sentences2)\n",
    "        pearson_correlation = round(scipy.stats.pearsonr(sims, df['sim'])[0] * 100,2)\n",
    "        #print(label, pearson_correlation)\n",
    "        pearson_cors.append(pearson_correlation)\n",
    "        \n",
    "    return pearson_cors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do STS evaluation\n",
    "\n",
    "Note that results below are a bit different from what has been reported in the appendix of our paper because we are using a small word2vec for demonstration purpose here -- there are more out-of-vocabulary words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS-2012-MSRvid\n",
      "STS-2014-images\n",
      "STS-2015-images\n",
      "STS-2014-deft-forum\n",
      "STS-2012-MSRpar\n",
      "STS-2014-deft-news\n",
      "STS-2013-headlines\n",
      "STS-2014-headlines\n",
      "STS-2015-headlines\n",
      "STS-2016-headlines\n",
      "STS-2017-track5.en-en\n",
      "STS-2015-answers-forums\n",
      "STS-2016-answer-answer\n",
      "STS-2012-surprise.OnWN\n",
      "STS-2013-FNWN\n",
      "STS-2013-OnWN\n",
      "STS-2014-OnWN\n",
      "STS-2014-tweet-news\n",
      "STS-2015-belief\n",
      "STS-2016-plagiarism\n",
      "STS-2016-question-question\n",
      "STS-2012-SMTeuroparl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liutianlin/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:1128: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis)\n",
      "/Users/liutianlin/anaconda3/lib/python3.6/site-packages/numpy/core/_methods.py:80: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STS-2012-surprise.SMTnews\n",
      "STS-2016-postediting\n"
     ]
    }
   ],
   "source": [
    "\n",
    "benchmarks = [(\"CN-glove\", ft.partial(run_conceptor_benchmark, model_str= 'cn_glove')),    \n",
    "             (\"CN-word2vec\", ft.partial(run_conceptor_benchmark, model_str= 'cn_word2vec'))]\n",
    "\n",
    "pearson_results_year_task = {}\n",
    "\n",
    "for year_task in sts_all['year-task'].unique():\n",
    "    print('STS-' + year_task)\n",
    "    pearson_results_year_task['STS-' + year_task] = run_experiment(sts_by_year_task[year_task], benchmarks)  \n",
    "    \n",
    "pearson_results_year_task['SICK'] = run_experiment(sick_all, benchmarks) \n",
    "# pearson_results_year_task['TWITTER'] = run_experiment(twitter_all, benchmarks) \n",
    "\n",
    "pearson_results_year_task['2015-answers_students'] = run_experiment(answers_students_2015, benchmarks) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CN-glove</th>\n",
       "      <th>CN-word2vec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>STS-2012-MSRpar</th>\n",
       "      <td>41.27</td>\n",
       "      <td>40.37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2012-MSRvid</th>\n",
       "      <td>62.50</td>\n",
       "      <td>75.22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2012-surprise.OnWN</th>\n",
       "      <td>67.87</td>\n",
       "      <td>70.82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2012-SMTeuroparl</th>\n",
       "      <td>52.58</td>\n",
       "      <td>35.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2012-surprise.SMTnews</th>\n",
       "      <td>47.69</td>\n",
       "      <td>50.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2013-FNWN</th>\n",
       "      <td>42.03</td>\n",
       "      <td>43.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2013-OnWN</th>\n",
       "      <td>57.45</td>\n",
       "      <td>68.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2013-headlines</th>\n",
       "      <td>67.00</td>\n",
       "      <td>64.78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-OnWN</th>\n",
       "      <td>66.43</td>\n",
       "      <td>75.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-deft-forum</th>\n",
       "      <td>37.57</td>\n",
       "      <td>42.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-deft-news</th>\n",
       "      <td>69.08</td>\n",
       "      <td>65.57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-headlines</th>\n",
       "      <td>61.71</td>\n",
       "      <td>61.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-tweet-news</th>\n",
       "      <td>75.37</td>\n",
       "      <td>74.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2014-images</th>\n",
       "      <td>65.81</td>\n",
       "      <td>78.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2015-answers-forums</th>\n",
       "      <td>48.62</td>\n",
       "      <td>53.66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-answers_students</th>\n",
       "      <td>69.68</td>\n",
       "      <td>71.45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2015-belief</th>\n",
       "      <td>59.77</td>\n",
       "      <td>61.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2015-headlines</th>\n",
       "      <td>69.18</td>\n",
       "      <td>68.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>STS-2015-images</th>\n",
       "      <td>71.43</td>\n",
       "      <td>80.48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SICK</th>\n",
       "      <td>66.42</td>\n",
       "      <td>72.34</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           CN-glove  CN-word2vec\n",
       "STS-2012-MSRpar               41.27        40.37\n",
       "STS-2012-MSRvid               62.50        75.22\n",
       "STS-2012-surprise.OnWN        67.87        70.82\n",
       "STS-2012-SMTeuroparl          52.58        35.14\n",
       "STS-2012-surprise.SMTnews     47.69        50.08\n",
       "STS-2013-FNWN                 42.03        43.99\n",
       "STS-2013-OnWN                 57.45        68.76\n",
       "STS-2013-headlines            67.00        64.78\n",
       "STS-2014-OnWN                 66.43        75.08\n",
       "STS-2014-deft-forum           37.57        42.80\n",
       "STS-2014-deft-news            69.08        65.57\n",
       "STS-2014-headlines            61.71        61.09\n",
       "STS-2014-tweet-news           75.37        74.55\n",
       "STS-2014-images               65.81        78.24\n",
       "STS-2015-answers-forums       48.62        53.66\n",
       "2015-answers_students         69.68        71.45\n",
       "STS-2015-belief               59.77        61.29\n",
       "STS-2015-headlines            69.18        68.88\n",
       "STS-2015-images               71.43        80.48\n",
       "SICK                          66.42        72.34"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# plt.rcParams['figure.figsize'] = (10,5)\n",
    "\n",
    "pearson_results_year_task_df = pd.DataFrame(pearson_results_year_task)\n",
    "pearson_results_year_task_df = pearson_results_year_task_df.transpose()\n",
    "pearson_results_year_task_df = pearson_results_year_task_df.rename(columns={i:b[0] for i, b in enumerate(benchmarks)})\n",
    "\n",
    "pearson_results_year_task_df.reindex(['STS-2012-MSRpar', 'STS-2012-MSRvid', 'STS-2012-surprise.OnWN', 'STS-2012-SMTeuroparl', 'STS-2012-surprise.SMTnews','STS-2013-FNWN', 'STS-2013-OnWN', 'STS-2013-headlines',  'STS-2014-OnWN', 'STS-2014-deft-forum','STS-2014-deft-news', 'STS-2014-headlines', 'STS-2014-tweet-news',  'STS-2014-images', 'STS-2015-answers-forums', '2015-answers_students', 'STS-2015-belief',  'STS-2015-headlines', 'STS-2015-images', 'SICK'])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment 3: Concept Categorization\n",
    "\n",
    "\n",
    "In the concept categorization task, we used k-means to cluster words into concept cate- gories based on their vector representations (for example, “bear” and “cat” belong to the concept category of animals). We use three standard datasets: (i) a rather small dataset ESSLLI 2008 (Baroni, Evert, and Lenci, 2008) that contains 44 concepts in 9 categories; (ii) the Almuhareb-Poesio (AP) (Poesio and Almuhareb, 2005), which contains 402 concepts divided into 21 categories; and (iii) the BM dataset (Bat- tig and Montague, 1969) that 5321 concepts divided into 56 categories. Note that the datasets of ESSLLI, AP, and BM are increasingly challenging for clustering algorithms, due to the increasing numbers of words and categories.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculate_purity(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate purity for given true and predicted cluster labels.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array, shape: (n_samples, 1)\n",
    "      True cluster labels\n",
    "    y_pred: array, shape: (n_samples, 1)\n",
    "      Cluster assingment.\n",
    "    Returns\n",
    "    -------\n",
    "    purity: float\n",
    "      Calculated purity.\n",
    "    \"\"\"\n",
    "    assert len(y_true) == len(y_pred)\n",
    "    true_clusters = np.zeros(shape=(len(set(y_true)), len(y_true)))\n",
    "    pred_clusters = np.zeros_like(true_clusters)\n",
    "    for id, cl in enumerate(set(y_true)):\n",
    "        true_clusters[id] = (y_true == cl).astype(\"int\")\n",
    "    for id, cl in enumerate(set(y_pred)):\n",
    "        pred_clusters[id] = (y_pred == cl).astype(\"int\")\n",
    "\n",
    "    M = pred_clusters.dot(true_clusters.T)\n",
    "    return 1. / len(y_true) * np.sum(np.max(M, axis=1))\n",
    "\n",
    "def evaluateCategorization(thisDict_str, testDataset_csv, method = 'fixed'):\n",
    "    \n",
    "    categorizationFile = resourceFile + 'word-categorization/monolingual/en/' + testDataset_csv\n",
    "\n",
    "    \n",
    "    thisDict = eval(thisDict_str)\n",
    "    modelVocab = list(thisDict.keys())\n",
    "\n",
    "    categorty_list = []\n",
    "    word_list = []\n",
    "\n",
    "    with open(categorizationFile, newline='') as csvfile:\n",
    "        next(csvfile)\n",
    "        reader = csv.reader(csvfile, quotechar='|')\n",
    "        for row in reader:\n",
    "            if len(row[2]) != 0 and row[2] in modelVocab:\n",
    "                categorty_list.append(row[1])\n",
    "                word_list.append(row[2])\n",
    "\n",
    "\n",
    "    wordVectorsMat = np.array([thisDict[word] for word in word_list])\n",
    "\n",
    "    initCentroids = []\n",
    "    for category in set(categorty_list):\n",
    "        indicesCategory = [i for i in range(len(categorty_list)) if categorty_list[i]== category]\n",
    "        initCentroid = np.mean(wordVectorsMat[indicesCategory, :], axis = 0)\n",
    "        initCentroids.append(initCentroid)\n",
    "\n",
    "    initCentroids = np.array(initCentroids)\n",
    "\n",
    "    if method == 'fixed':\n",
    "    \n",
    "        predClusters = KMeans(init = initCentroids, n_clusters=len(set(categorty_list))).fit_predict(wordVectorsMat)\n",
    "        purity = calculate_purity(np.array(categorty_list), predClusters)\n",
    "\n",
    "    else:\n",
    "        \n",
    "        predClusters = KMeans(n_init=10000, n_clusters=len(set(categorty_list))).fit_predict(wordVectorsMat)\n",
    "        purity= calculate_purity(np.array(categorty_list), predClusters)\n",
    "        \n",
    "        \n",
    "    return purity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wordVecBrands_methods = ['cn_word2vec', 'cn_glove'] \n",
    "csvFile = ['battig.csv', 'ap.csv', 'essli-2008.csv']\n",
    "\n",
    "\n",
    "c = list(itertools.product(csvFile, wordVecBrands_methods))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cn_word2vec-battig.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liutianlin/anaconda3/lib/python3.6/site-packages/sklearn/cluster/k_means_.py:896: RuntimeWarning: Explicit initial center position passed: performing only one init in k-means instead of n_init=10\n",
      "  return_n_iter=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.19\n",
      "cn_glove-battig.csv\n",
      "67.63\n",
      "cn_word2vec-ap.csv\n",
      "89.31\n",
      "cn_glove-ap.csv\n",
      "90.95\n",
      "cn_word2vec-essli-2008.csv\n",
      "100.0\n",
      "cn_glove-essli-2008.csv\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "all_purity = []\n",
    "for (csvFile, wordVecBrand_method) in c:\n",
    "    print(wordVecBrand_method + '-' + csvFile)\n",
    "    print(round(evaluateCategorization(wordVecBrand_method, csvFile) * 100,2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
